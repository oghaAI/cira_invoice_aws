# Story 3.1: OpenAI Integration via AI SDK

## Status
Draft

## Story
As a LLM processor,
I want a working GPT-4 connection using the AI SDK,
so that I can extract structured data from OCR text reliably.

## Acceptance Criteria
1. Configure Azure OpenAI connection with API key and endpoint (no secrets in code).
2. Implement GPT-4 client via `ai` SDK with support for structured outputs (to be used with Zod in Story 3.2).
3. Add request timeouts, retries (with backoff), and basic rate limiting guards.
4. Create a reusable prompt template for invoice extraction (system + user messages) without sensitive data leaks.
5. Emit structured logs for requests/responses (no content leakage; include timings, model, tokens when available).
6. Gracefully map API failures to typed errors (validation/auth/quota/timeout/server) for Step Functions retries.

## Tasks / Subtasks
- [ ] Task 1: Provider Configuration (AC: 1)
  - [ ] Add env vars to LLM Lambda in `packages/infrastructure/src/stacks/api-stack.ts`:
    - `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_DEPLOYMENT` (model name), `AZURE_OPENAI_API_VERSION`
  - [ ] Validate presence at runtime; fail fast with clear error
  - [ ] Update README/env sample if applicable

- [ ] Task 2: AI SDK Client Wrapper (AC: 2, 3, 5, 6)
  - [ ] Create `packages/api/src/services/llm/client.ts` exporting `getLlmClient()` and `callLlm()` helpers using `ai` SDK
  - [ ] Support Azure OpenAI via fetch-compatible client; set base URL and headers
  - [ ] Add timeout via `AbortController` (default 30s, configurable)
  - [ ] Add retry/backoff utility for transient errors (429/5xx) with jitter
  - [ ] Emit structured logs: `{ model, durationMs, attempt, status, tokens? }` without prompt/response content
  - [ ] Map errors to unified categories: `VALIDATION`, `AUTH`, `QUOTA`, `TIMEOUT`, `SERVER`, `FAILED_STATUS`

- [ ] Task 3: Prompt Template (AC: 4)
  - [ ] Create `packages/api/src/services/llm/prompts/invoice.ts` with system prompt and user prompt template accepting markdown input
  - [ ] Include content safety: instruct model to only extract, not infer; output strictly structured JSON
  - [ ] Keep prompt free of tenant identifiers; accept `clientId` separately if needed for logs only

- [ ] Task 4: Wiring in Handler (Scaffold Only) (AC: 2, 5)
  - [ ] Update `packages/api/src/handlers/llm-extraction.ts` to call `callLlm()` with placeholder schema until Story 3.2
  - [ ] Ensure handler returns minimal object to Step Functions and does not log PII

- [ ] Task 5: Tests (AC: 3, 5, 6)
  - [ ] Unit: timeouts trigger abort and proper error mapping
  - [ ] Unit: retries with backoff on 429/5xx; no retry on 400/401/403
  - [ ] Unit: log fields present (no prompt/response bodies)
  - [ ] Unit: env validation fails fast

## Dev Notes
- Library: use `ai` (AI SDK) for provider-agnostic LLM calls; provider is Azure OpenAI.
- Keep the client thin and side-effect free; pass AbortSignal for cancellation.
- No secrets in logs; never log raw prompts or outputs.
- Step Functions error handling relies on typed errors for retry policy.

## Testing
- Mock fetch/AI SDK client to simulate 200/400/401/403/429/500 and timeout conditions.
- Verify abort controller path and that retries honor max attempts and backoff.

## File List
- packages/api/src/services/llm/client.ts
- packages/api/src/services/llm/prompts/invoice.ts
- packages/api/src/handlers/llm-extraction.ts
- packages/infrastructure/src/stacks/api-stack.ts
- docs/stories/3.1.openai-integration-ai-sdk.md

## Change Log
- 2025-09-14: Drafted story with AI SDK focus and reliability guards.

## QA Results

